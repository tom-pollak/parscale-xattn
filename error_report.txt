# Error Report: parscale-xattn

This report summarizes the unique errors found in the test suite, categorized by difficulty. Each entry includes a representative stack trace and a pointer to the code that needs to be fixed.

---

## इ Easy (Easy Fixes)

### 1. Incorrect Default Configuration

**Issue:** Creating a default configuration object (`parscale_n=1`) raises a `ValueError` because `parscale_n_tokens` defaults to a non-zero value, which is an invalid combination for the standard (non-ParScale) mode. This is the most frequent error, causing 9 test failures.

**File to Fix:** `src/parscale_xattn/config_base.py`

**Suggested Change:** In the `ParScaleBaseConfig` class, change the default value of `parscale_n_tokens` from 48 to 0.

**Stack Trace:**
```
_______________________________________________________________________ TestAPICompatibility.test_original_class_names_available _______________________________________________________________________

self = <tests.compatibility.test_backward_compat.TestAPICompatibility object at 0x7e29318d5b50>

    def test_original_class_names_available(self):
        """Test that original class names are still available as aliases."""
        # These should all be importable and work
>       config = Qwen2ParScaleConfig(parscale_n=1)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/compatibility/test_backward_compat.py:28: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/parscale_xattn/configuration_qwen2_parscale.py:143: in __init__
    super().__init__(**kwargs)
src/parscale_xattn/config_cross_attn.py:46: in __init__
    super().__init__(**kwargs)
src/parscale_xattn/config_base.py:138: in __init__
    self._validate_parscale_base_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[ValueError('Prefix tokens should be 0 when parscale_n=1 (standard Qwen2 mode), but parscale_n_tokens=48') raised in repr()] Qwen2ParScaleConfig object at 0x7e2924983f80>

    def _validate_parscale_base_config(self):
        """Validate base ParScale configuration parameters."""
        # Basic bounds checking
        if self.parscale_n < 1:
            raise ValueError(f"parscale_n must be >= 1, got {self.parscale_n}")
    
        if self.parscale_n_tokens < 0:
            raise ValueError(
                f"parscale_n_tokens must be >= 0, got {self.parscale_n_tokens}"
            )
    
        # When parscale_n=1, enforce standard Qwen2 behavior
        if self.parscale_n == 1:
            if self.parscale_n_tokens > 0:
>               raise ValueError(
                    f"Prefix tokens should be 0 when parscale_n=1 (standard Qwen2 mode), "
                    f"but parscale_n_tokens={self.parscale_n_tokens}"
                )
E               ValueError: Prefix tokens should be 0 when parscale_n=1 (standard Qwen2 mode), but parscale_n_tokens=48

src/parscale_xattn/config_base.py:159: ValueError
```

### 2. Missing `repeat` Import

**Issue:** The model's forward pass fails with a `NameError` because the `repeat` function from `einops` is used without being imported. This affects 11 tests.

**File to Fix:** `src/parscale_xattn/modeling_cross_attn.py`

**Suggested Change:** Add `from einops import repeat` at the top of the file.

**Stack Trace:**
```
__________________________________________________________________ TestParScaleModeCompatibility.test_parscale_forward_batch_handling __________________________________________________________________

self = <tests.compatibility.test_backward_compat.TestParScaleModeCompatibility object at 0x7e2924b3fa70>

    def test_parscale_forward_batch_handling(self):
        """Test that ParScale mode handles batches correctly."""
        config = Qwen2ParScaleConfig(
            parscale_n=4,
            parscale_n_tokens=24,
            hidden_size=64,
            num_hidden_layers=1,
            vocab_size=100
        )
        model = Qwen2ParScaleForCausalLM(config)
    
        # Test different batch sizes
        for batch_size in [1, 2, 4]:
            seq_len = 3
            input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    
>           output = model(input_ids)
                     ^^^^^^^^^^^^^^^^

tests/compatibility/test_backward_compat.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: in _call_impl
    return forward_call(*args, **kwargs)
src/parscale_xattn/modeling_cross_attn.py:450: in forward
    outputs = self.model(
.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ParScaleCrossAttnModel(...)
# ... (omitted for brevity) ...
    
        if self.parscale_n > 1:
            # Input transformation: we directly copy the input for n_parscale times.
>           inputs_embeds = repeat(
                            ^^^^^^
                inputs_embeds, "b s h -> (n_parscale b) s h", n_parscale=self.parscale_n
            )
E           NameError: name 'repeat' is not defined

src/parscale_xattn/modeling_cross_attn.py:194: NameError
```

---

## मध्यम (Medium)

### 3. Incorrect Test Validation Logic

**Issue:** Two unit tests for configuration validation are failing. They are designed to catch specific errors but are instead being tripped up by the "Incorrect Default Configuration" error (Easy #1). The test setup needs to be corrected to properly test the intended validation logic.

**File to Fix:** `tests/unit/test_config.py`

**Suggested Change:** In `test_cross_attn_validation` and `test_replica_rope_validation`, adjust the `ParScaleCrossAttnConfig` instantiation to prevent the initial `ValueError`. For example, explicitly set `parscale_n_tokens=0` when `parscale_n=1`.

**Stack Trace:**
```
________________________________________________________________________ TestParScaleCrossAttnConfig.test_cross_attn_validation ________________________________________________________________________

self = <tests.unit.test_config.TestParScaleCrossAttnConfig object at 0x7e29249d84d0>

    def test_cross_attn_validation(self):
        """Test cross-attention specific validation."""
        # Cross-attention requires parscale_n > 1
>       with pytest.raises(ValueError, match="Cross-attention.*requires parscale_n > 1"):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Regex pattern did not match.
E        Regex: 'Cross-attention.*requires parscale_n > 1'
E        Input: 'Prefix tokens should be 0 when parscale_n=1 (standard Qwen2 mode), but parscale_n_tokens=48'

tests/unit/test_config.py:98: AssertionError
```

### 4. Incorrect Tensor Manipulation in Test

**Issue:** The unit test `test_input_replication` fails because it uses `einops.rearrange` to test input replication logic, but the error message reveals that `rearrange` cannot introduce new identifiers (`n_parscale`) on the right side of the expression. The model itself should be using `einops.repeat`.

**File to Fix:** `tests/unit/test_output_aggregation.py`

**Suggested Change:** In `test_input_replication`, replace the call to `rearrange` with `repeat` to correctly test the expected behavior.

**Stack Trace:**
```
_____________________________________________________________________________ TestOutputAggregation.test_input_replication _____________________________________________________________________________

self = <tests.unit.test_output_aggregation.TestOutputAggregation object at 0x7e29249d9d30>

    def test_input_replication(self):
        """Test that inputs are correctly replicated across replicas."""
        input_ids = torch.randint(0, self.config.vocab_size, (self.batch_size, self.seq_len))
    
        # Get input embeddings
        inputs_embeds = self.model.embed_tokens(input_ids)
        original_shape = inputs_embeds.shape
    
        # Should be replicated to (parscale_n * batch_size, seq_len, hidden_size)
>       replicated_embeds = rearrange(
            inputs_embeds, "b s h -> (n_parscale b) s h", n_parscale=self.config.parscale_n
        )

tests/unit/test_output_aggregation.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.12/site-packages/einops/einops.py:600: in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
.venv/lib/python3.12/site-packages/einops/einops.py:542: in reduce
    raise EinopsError(message + "\n {}".format(e))
E   einops.EinopsError:  Error while processing rearrange-reduction pattern "b s h -> (n_parscale b) s h".
E    Input tensor shape: torch.Size([2, 5, 128]). Additional info: {'n_parscale': 4}.
E    Identifiers only on one side of expression (should be on both): {'n_parscale'}
```

---

## कठिन (Hard)

### 5. Attention Mechanism Runtime Error

**Issue:** An integration test fails with a `RuntimeError` in the `scaled_dot_product_attention` function. The error message `The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1` indicates a shape mismatch between the query and key tensors. The key tensor has a sequence length of 0, suggesting the KV cache is not being populated correctly before being used in the attention calculation.

**File to Fix:** `src/parscale_xattn/modeling_base.py` (likely in the cache management logic within the decoder/attention layers).

**Suggested Change:** This requires significant debugging. The logic for updating and passing the `past_key_values` cache needs to be inspected to understand why an empty tensor is being passed to the attention mechanism.

**Stack Trace:**
```
________________________________________________________________ TestCrossAttentionNonInterference.test_cross_attn_disabled_equals_base ________________________________________________________________

self = <tests.integration.test_equivalence.TestCrossAttentionNonInterference object at 0x7e2924983260>
# ... (omitted for brevity) ...
        with torch.no_grad():
>           base_output = base_model(input_ids)
                          ^^^^^^^^^^^^^^^^^^^^^

tests/integration/test_equivalence.py:277: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:66: RuntimeError
```

### 6. Other Complex Logic Failures

Multiple tests indicate deeper bugs in the model's core features. These do not share a single stack trace but point to related areas of complex logic.

*   **`TestParscaleCache::test_cache_reorder_for_beam_search` (FAILED):** The logic to reorder KV cache tensors for beam search is incorrect.
*   **`TestAttentionMaskExpansion::test_attention_mask_expansion` (FAILED):** The attention mask is not being correctly expanded to account for prefix tokens, which can lead to incorrect attention scores.
*   **`TestAggregationMathematicalProperties::test_aggregation_deterministic` (FAILED):** The output aggregation mechanism is not deterministic, which is a critical flaw for reproducibility and stable training.

**Files to Fix:**
*   `src/parscale_xattn/modeling_base.py`
*   `src/parscale_xattn/modeling_cross_attn.py`
*   `src/parscale_xattn/cross_attention.py`

**Suggested Change:** Each of these failures requires careful, separate debugging of the specific feature implementation.
